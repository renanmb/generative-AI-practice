{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Renan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# advanced GAN\n",
    "# Convolutional layer hacking with Python and Numpy\n",
    "# https://towardsdatascience.com/convolutional-layer-hacking-with-python-and-numpy-e5f64812ca0c\n",
    "# # https://medium.com/@ideami\n",
    "#import the libraries\n",
    "\n",
    "import torch, torchvision, os, PIL, pdb\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "def show(tensor, num=25, wamb=0, name=''):\n",
    "    data = tensor.detach().cpu()\n",
    "    grid = make_grid(data[:num], nrows=5).permute(1,2,0)\n",
    "\n",
    "    # optional - Weights and Biases\n",
    "    if (wandb == 1 and wandbact ==1):\n",
    "        wandb.log({name:wandb.Image(grid.numpy().clip(0,1))})\n",
    "    \n",
    "    plt.imshow(grid.clip(0,1))\n",
    "    plt.show\n",
    "\n",
    "n_epochs = 10000\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "z_dim = 200\n",
    "device='cuda'\n",
    "\n",
    "cur_step = 0\n",
    "# critique cycles of training\n",
    "crit_cycles=5 \n",
    "gen_losses=[]\n",
    "crit_losses=[]\n",
    "show_step=35\n",
    "save_step=35\n",
    "# Track stats through weights and biases\n",
    "wandbact = 1 \n",
    "\n",
    "wandb.login(key='708ae0ba329cae8bf5ba95fbdbacd445fe02de5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "experiment_name = wandb.util.generate_id()\n",
    "\n",
    "myrun=wandb.init(\n",
    "    project=\"wgan\",\n",
    "    group=experiment_name,\n",
    "    config={\n",
    "        \"optimizer\":\"adam\",\n",
    "        \"model\":\"wgan gp\",\n",
    "        \"epoch\":\"1000\",\n",
    "        \"batch_size\":128\n",
    "    }\n",
    ")\n",
    "\n",
    "config=wandb.config\n",
    "\n",
    "## Sometimes this call to the wandb service may fail or give problems.\n",
    "## If needed, you can use the Runtime->Restart Runtime option in the menus\n",
    "## to restart and try again. If you keep having issues, you may set wandbact=0\n",
    "## to skip temporarily the remote stats. You would also\n",
    "## need to set wandbactive to 0 within the training loop in\n",
    "## show(fake, wandbactive=0, name='fake') and show(real, wandbactive=0, name='real').\n",
    "## However, in general this should work with no problems (try a few times if you have issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49462ozz\n"
     ]
    }
   ],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator model\n",
    "\n",
    "from turtle import forward\n",
    "\n",
    "\n",
    "class Generator(nn.Model):\n",
    "    def __init__(self, z_dim=64, d_dim=16) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim=z_dim\n",
    "\n",
    "        self.gen = nn.Sequential(\n",
    "            # ConvTranspose2d: in_channels, out_channels, kernel_size, stride=1, padding=0\n",
    "            #  Calculating new idth and height: (n-1)*stride -2*padding + ks\n",
    "            #  n = width or height\n",
    "            #  ks = kernel size\n",
    "            #  we begin with a 1x1 image with z_dim number of channels (200)\n",
    "            nn.ConvTranspose2d(z_dim, d_dim*32, 4, 1, 0), # 4x4 (ch: 200, 512)\n",
    "            nn.BatchNorm2d(d_dim*32),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(d_dim*32, d_dim*16, 4, 2, 1), # 8x8 (ch: 512, 256)\n",
    "            nn.BatchNorm2d(d_dim*16),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(d_dim*16, d_dim*8, 4, 2, 1), # 16x16 (ch: 256, 128)\n",
    "            # (n-1)*stride -2*padding + ks = (8-1)*2-2*1+4 = 16\n",
    "            nn.BatchNorm2d(d_dim*8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(d_dim*8, d_dim*4, 4, 2, 1), # 32x32 (ch: 128, 64)\n",
    "            nn.BatchNorm2d(d_dim*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(d_dim*4, d_dim*2, 4, 2, 1), # 64x64 (ch: 64, 32)\n",
    "            nn.BatchNorm2d(d_dim*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(d_dim*2, 3, 4, 2, 1), # 128x128 (ch: 32, 3)\n",
    "            nn.Tanh() #produce result in the range from -1 to 1\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x=noise.view(len(noise), self.z_dim, 1 , 1) # 128 x 200 x1 x 1\n",
    "        return self.gen(x)\n",
    "\n",
    "def gen_noise(num, z_dim, device='cuda'):\n",
    "    return torch.randn(num, z_dim, device=device) # 128 x 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Critique model\n",
    "# Conv2D: in_channels, out_channels, kernel_size, stride=1, apdding=0\n",
    "# New width and height: # (n+2*pad-ks)//stride +1\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, d_dim=16) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        self.crit = nn.Sequential(\n",
    "            nn.Conv2d(3,d_dim,4,2,1), # (n+2*pad-ks)//stride +1 = (128+2*1-4)//2+1=64x64 (ch: 3,16)\n",
    "            nn.InstanceNorm2d(d_dim), # normalizing by instance works best for Critic\n",
    "            nn.LeakyReLU(0.2), # has a little of a negative slope, help to preserve signal\n",
    "\n",
    "            nn.Conv2d(d_dim, d_dim*2,4,2,1), # 32x32 (ch: 16,32)\n",
    "            nn.InstanceNorm2d(d_dim*2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(d_dim*2, d_dim*4,4,2,1), # 16x16 (ch: 32,64)\n",
    "            nn.InstanceNorm2d(d_dim*4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(d_dim*4, d_dim*8,4,2,1), # 8x8 (ch: 64,128)\n",
    "            nn.InstanceNorm2d(d_dim*8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(d_dim*8, d_dim*16,4,2,1), # 4x4 (ch: 128,256)\n",
    "            nn.InstanceNorm2d(d_dim*16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(d_dim*16, 1,4,1,0), # (n+2*pad-ks)//stride +1 = (4+2*0-4)//1+1 = 1x1 (ch: 256,1)\n",
    "\n",
    "        ) \n",
    "\n",
    "    def forward(self, image):\n",
    "        # iamge: 128 x 3 x128 x 128\n",
    "        crit_pred = self.critic(image) # 128 x 1 x 1 x 1\n",
    "        return crit_pred.view(len(crit_pred), -1) # 128 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  optional, init your weights in different ways\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight,0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias,0)\n",
    "    \n",
    "# gen = gen.apply(init_weights)\n",
    "# crit = crit.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# TODO make it work locally\n",
    "# gdown - Download a large file from Google Drive.\n",
    "import gdown, zipfile\n",
    "url = 'https://drive.google.com/'\n",
    "path='data/celeba'\n",
    "download_path=f'{path}/img_align_celeba.zip'\n",
    "\n",
    "if not os.path.exist(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "gdown.download(url, download_path, quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(download_path, 'r') as ziphandler:\n",
    "    ziphandler.extractall(path)\n",
    "\n",
    "#### Alternative download address:\n",
    "# Celebra gdrive: https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg?resourcekey=0-rJlzl934LzC-Xp28GeIBzQ\n",
    "# Kaggle: https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, Dataloader, declare gen, crit, test dataset\n",
    "\n",
    "class Dataset(Dataset):\n",
    "  def __init__(self, path, size=128, lim=10000):\n",
    "    self.sizes=[size, size]\n",
    "    items, labels=[],[]\n",
    "\n",
    "    for data in os.listdir(path)[:lim]:\n",
    "      #path: './data/celeba/img_align_celeba'\n",
    "      #data: '114568.jpg\n",
    "      item = os.path.join(path,data)\n",
    "      items.append(item)\n",
    "      labels.append(data)\n",
    "    self.items=items\n",
    "    self.labels=labels\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.items)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    data = PIL.Image.open(self.items[idx]).convert('RGB') # (178,218)\n",
    "    data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # 128 x 128 x 3\n",
    "    data = np.transpose(data, (2,0,1)).astype(np.float32, copy=False) # 3 x 128 x 128 # from 0 to 255\n",
    "    data = torch.from_numpy(data).div(255) # from 0 to 1\n",
    "    return data, self.labels[idx]\n",
    "\n",
    "## Dataset\n",
    "data_path='./data/celeba/img_align_celeba'\n",
    "ds = Dataset(data_path, size=128, lim=10000)\n",
    "\n",
    "## DataLoader\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## Models\n",
    "gen = Generator(z_dim).to(device)\n",
    "crit = Critic().to(device)\n",
    "\n",
    "## Optimizers\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(0.5,0.9))\n",
    "crit_opt= torch.optim.Adam(crit.parameters(), lr=lr, betas=(0.5,0.9))\n",
    "\n",
    "## Initializations\n",
    "##gen=gen.apply(init_weights)\n",
    "##crit=crit.apply(init_weights)\n",
    "\n",
    "#wandb optional\n",
    "if (wandbact==1):\n",
    "  wandb.watch(gen, log_freq=100)\n",
    "  wandb.watch(crit, log_freq=100)\n",
    "\n",
    "x,y=next(iter(dataloader))\n",
    "show(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  gradient penalty calculation\n",
    "\n",
    "def get_gp(real, fake, crit, alpha, gamma=10):\n",
    "    mix_images = real * alpha + fake * (1-alpha) # 128 x 3 x 128 x 128\n",
    "    mix_scores = crit(mix_images) # 128 x 1\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = mix_images,\n",
    "        outputs = mix_scores,\n",
    "        grad_outputs=torch.ones_like(mix_scores),\n",
    "        retain_graph=True,\n",
    "        create_graph=True,\n",
    "  )[0] # 128 x 3 x 128 x 128\n",
    "\n",
    "    gradient = gradient.view(len(gradient), -1)   # 128 x 49152\n",
    "    gradient_norm = gradient.norm(2, dim=1) \n",
    "    gp = gamma * ((gradient_norm-1)**2).mean()\n",
    "\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Checkpoints\n",
    "\n",
    "root_path='./data/'\n",
    "\n",
    "def save_checkpoint(name):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': gen.state_dict(),\n",
    "        'optimizer_state_dict': gen_opt.state_dict() \n",
    "        }, f\"{root_path}G-{name}.pkl\"\n",
    "    )\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': crit.state_dict(),\n",
    "        'optimizer_state_dict': crit_opt.state_dict()\n",
    "        }, f\"{root_path}C-{name}.pk1\"\n",
    "    )\n",
    "    print(\"saved checkpoint\")\n",
    "\n",
    "def load_checkpoint(name):\n",
    "    checkpoint = torch.load(f\"{root_path}G-{name}.pk1\")\n",
    "    gen.load_state_dict(checkpoint['model_state_dict'])\n",
    "    gen_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    checkpoint = torch.load(f\"{root_path}C-{name}.pkl\")\n",
    "    crit.load_state_dict(checkpoint['model_state_dict'])\n",
    "    crit_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    print(\"Loaded checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(n_epochs):\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_bs = len(real) # 128\n",
    "        real = real.to(device)\n",
    "    \n",
    "    # CRITIC\n",
    "    mean_critic_loss = 0\n",
    "    for _ in range(crit_cycles):\n",
    "        crit_opt.zero_grad()\n",
    "\n",
    "        noise=gen_noise(cur_bs, z_dim)\n",
    "        fake = gen(noise)\n",
    "        crit_fake_pred = crit(fake.detach())\n",
    "        crit_real_pred = crit(real)\n",
    "\n",
    "        alpha = torch.rand(len(real),1,1,1,device=device,requires_grad=True) # 128x1x1x1\n",
    "        gp = get_gp(real, fake.detach(), crit, alpha)\n",
    "\n",
    "        crit_loss = crit_fake_pred.mean() - crit_real_pred.mean() + get_gp\n",
    "\n",
    "        mean_critic_loss += crit_loss.item() / crit_cycles\n",
    "\n",
    "        crit_loss.backward(retain_graph=True)\n",
    "        crit_opt.step()\n",
    "\n",
    "    crit_losses+=[mean_critic_loss]\n",
    "\n",
    "    #  Generator\n",
    "    gen_opt.zero_grad()\n",
    "    noise = gen_noise(cur_bs, z_dim)\n",
    "    fake = gen(noise)\n",
    "    crit_fake_pred = crit(fake)\n",
    "\n",
    "    gen_loss = -crit_fake_pred.mean()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "\n",
    "    gen_losses=+[gen_loss.item()]\n",
    "\n",
    "    # stats\n",
    "    if (wandbact == 1):\n",
    "        wandb.log({'Epoch':epoch, 'Step':cur_step, 'Critic loss':mean_critic_loss, \n",
    "        'Gen loss':gen_loss})\n",
    "    \n",
    "    if cur_step % save_step == 0 and cur_step > 0:\n",
    "        print(\"Saving checkpoint: \", cur_step, save_step)\n",
    "        save_checkpoint(\"latest\")\n",
    "        \n",
    "    if (cur_step % show_step == 0 and cur_step >0):\n",
    "        show(fake, wandbactive=1, name='dake')\n",
    "        show(real, wandbactive=1, name='real')    \n",
    "\n",
    "        gen_mean=sum(gen_losses[-show_step:]) / show_step\n",
    "        crit_mean = sum(crit_losses[-show_step]) / show_step\n",
    "        print(f\"Epoch: {epoch}: Step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n",
    "\n",
    "        plt.plot(\n",
    "            range(len(gen_losses)),\n",
    "            torch.Tensor(gen_losses),\n",
    "            label(\"Generator Loss\")\n",
    "        ) \n",
    "        plt.plot(\n",
    "            range(len(gen_losses)),\n",
    "            torch.Tensor(crit_losses),\n",
    "            label(\"Critic Loss\")\n",
    "        )\n",
    "        plt.ylim(-1000,1000)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    cur_step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate new faces\n",
    "noise = gen_noise(batch_size, z_dim)\n",
    "fake = gen(noise)\n",
    "show(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to select a specific image\n",
    "plt.imshow(fake[16].detach().cpu().permute(1,2,0).squeeze().clip(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MORPHING, interpolation between points in latent space\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "gen_set=[]\n",
    "z_shape=[1,200,1,1]\n",
    "rows=4\n",
    "steps=17\n",
    "\n",
    "for i in range(rows):\n",
    "    z1,z2 = torch.random(z_shape), torch.randn(z_shape)\n",
    "    for alpha in np.linspace(0,1,steps):\n",
    "        z=alpha*z1 + (1-alpha)*z2\n",
    "        res=gen(z.cuda())[0]\n",
    "        gen_set.append(res)\n",
    "\n",
    "fig = plt.figure(figsize=(25,11))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(rows,steps), axes_pad=0.1)\n",
    "\n",
    "for ax , img in zip (grid, gen_set):\n",
    "    ax.axis('off')\n",
    "    res=img.cpu().detach().permute(1,2,0)\n",
    "    res=res-res.min()\n",
    "    res=res/(res.max()-res.min())\n",
    "    ax.imshow(res.clip(0,1.0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ConvTranspose2d: (n-1)*stride -2*padding + ks\n",
    "nn.Conv2d: (n=2*pad-ks)//stride +1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa8dac3977e393e54a04f9eb0a99abe98dab145e1b09376ea10679288fb70c65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
